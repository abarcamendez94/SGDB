# Pipeline de Ingesta y Optimizaci√≥n de Portafolio (S&P 500, FTSE 100 y ETFs)

Este repositorio contiene la implementaci√≥n del modelo f√≠sico y la l√≥gica de ingesta para un sistema de optimizaci√≥n de portafolio con horizontes de inversi√≥n de medio/largo plazo y rebalanceo mensual.

## üìÇ Estructura del Proyecto

* **`/data`**: Contiene el motor de ingesta en Python (`tabla_1_ingesta.py`) y el archivo semilla de tickers (`tickers.csv`).
* **`/sql`**: Scripts DDL y DML para la limpieza de datos, auditor√≠a y creaci√≥n del universo final de inversi√≥n.
* **`/docs`**: Documentaci√≥n t√©cnica del modelado de datos y arquitectura (SQL + Parquet/Forward Fill).

## üõ†Ô∏è Requisitos Previos

Aseg√∫rese de tener instalado Python 3.10+ y las siguientes librer√≠as:
```bash
pip install yfinance pandas requests

## Diccionario de Tablas y Flujo de Datos

El pipeline procesa la informaci√≥n a trav√©s de tres estados l√≥gicos, representados por las siguientes tablas en SQLite:

1. **`tabla_1_ingesta`**: Tabla primaria generada por el motor de Python. Contiene los datos crudos de todos los tickers descargados.
2. **`tabla_1_limpia`**: Capa intermedia creada mediante SQL donde se normalizan los sectores (limpieza de 'Unknown') y se unifican criterios regionales.
3. **`universo_final`**: Producto final del modelo f√≠sico. Contiene el subset optimizado de ~200 activos que cumplen con los filtros de liquidez (Volumen) y solvencia (AUM) para el modelo XGBoost.

### ‚ö†Ô∏è Nota Importante sobre Nomenclatura
Para que los scripts SQL funcionen correctamente, aseg√∫rese de que la tabla inicial en SQLite tenga el nombre exacto: `tabla_1_ingesta`. 

* **Si usa el Script de Python:** El motor ya est√° programado para crear la tabla con este nombre autom√°ticamente dentro de `investigacion_tfm.db`.
* **Si importa manualmente un CSV:** Al usar el asistente de importaci√≥n de SQLite Browser, debe renombrar la tabla de destino a `tabla_1_ingesta` en el campo "Table name".

## üöÄ Instrucciones para levantar el Dataset en Local

Siga estos pasos en orden para reproducir el entorno de datos y generar la base de datos de investigaci√≥n:

## Paso 1: Ingesta de Datos (Capa Bronze/Silver)
Este paso descarga los datos hist√≥ricos desde Yahoo Finance y FRED, gestiona los valores nulos mediante *Forward Fill* y consolida la informaci√≥n macroecon√≥mica.
1. Abra su terminal en la carpeta ra√≠z del proyecto.
2. Ejecute el motor de ingesta:
   ```bash
   python data/tabla_1_ingesta.py
3. Resultado: Se creara el archivo data/investigacion_tfm.db (~135MB)

## Paso 2: Transformaci√≥n y Calidad
Una vez generada la base de datos, se aplica la l√≥gica de negocio y los filtros de calidad (AUM, Volumen y Supervivencia hist√≥rica) mediante SQL:
1. Conecte su gestor de base de datos (SQLite Browser, DBeaver o la extensi√≥n de VS Code) al archivo investigacion_tfm.db y el archivo tickers.csv
    1.1 Al abrir el archivo tickers.csv asegurarse de seleccionar y darle OK a "Column Names in Fist Line"
2. Ejecute el script de auditor√≠a para verificar la integridad:
   -- Ejecutar contenido de:
sql/01_auditoria_datos.sql
3. Ejecute el script de creaci√≥n del universo final de 200 activos:
-- Ejecutar contenido de:
sql/02_creacion_universo.sql

## Modelo Fisico (SFBD)
La implementaci√≥n del modelo f√≠sico se basa en una arquitectura relacional sobre SQLite, dise√±ada para soportar el rebalanceo mensual de la cartera:
Entidades Principales: * tabla_1_ingesta: Datos crudos (Raw Data) con precios ajustados, volumen y macro (CPI/Inflation).

tabla_1_limpia: Capa de datos curados con sectores normalizados.

universo_final: Vista materializada de los activos que cumplen los criterios de inversi√≥n para el modelo XGBoost.

Restricciones: Se aplican filtros de supervivencia (m√≠nimo 2,300 d√≠as de historia) para evitar el sesgo de supervivencia en el entrenamiento del modelo de Big Data.